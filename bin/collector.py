import requests
from bs4 import BeautifulSoup
import json
import datetime
import os
import random
import time
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed


# --- 1. ç½‘ç»œä¸ä»£ç†é…ç½® (è‡ªåŠ¨é€‚é… WSL) ---
def get_host_ip():
    """è‡ªåŠ¨è·å– WSL å®¿ä¸»æœº IPï¼Œè§£å†³ Connection Refused é—®é¢˜"""
    try:
        # è¯»å–é»˜è®¤ç½‘å…³
        result = subprocess.check_output(
            "ip route show | grep default", shell=True
        ).decode()
        return result.split()[2]
    except:
        return "127.0.0.1"


# è‡ªåŠ¨é…ç½®ä»£ç†
HOST_IP = get_host_ip()
PROXY_PORT = "7890"  
PROXY_URL = f"http://{HOST_IP}:{PROXY_PORT}"

print(f">>> [Init] å®¿ä¸»æœºIP: {HOST_IP}, ä»£ç†åœ°å€: {PROXY_URL}")

# --- 2. åŸºç¡€é…ç½® ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data/raw")
DATE_STR = datetime.datetime.now().strftime("%Y-%m-%d")

if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)


def get_proxies():
    return {"http": PROXY_URL, "https": PROXY_URL}


def get_random_ua():
    uas = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    ]
    return random.choice(uas)


def parse_stars_text(text):
    """è§£æ '1.2k' -> 1200"""
    if not text:
        return 0
    text = text.strip().replace(",", "").lower()
    try:
        if "k" in text:
            return int(float(text.replace("k", "")) * 1000)
        return int(text)
    except:
        return 0


# --- 3. æ ¸å¿ƒå¼•æ“ A: çˆ¬è™« (é«˜ç²¾åº¦) ---
def engine_scrape(period):
    """
    å°è¯•ä» HTML é¡µé¢çˆ¬å–æ•°æ®
    ä¼˜åŠ¿ï¼šèƒ½è·å– 'stars today' (çœŸå®å¢é‡)
    åŠ£åŠ¿ï¼šä¸ç¨³å®šï¼Œæ˜“è¢«åçˆ¬
    """
    print(f"[Engine-A] æ­£åœ¨çˆ¬å–: {period}...")
    url = f"https://github.com/trending?since={period}"

    # æœ€å¤šé‡è¯• 2 æ¬¡ï¼Œé¿å…æµªè´¹æ—¶é—´
    for attempt in range(1, 3):
        try:
            # éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹ŸçœŸäºº
            time.sleep(random.uniform(1, 2))

            resp = requests.get(
                url,
                headers={
                    "User-Agent": get_random_ua(),
                    "Accept-Language": "en-US,en;q=0.9",
                },
                proxies=get_proxies(),
                timeout=15,  # 15ç§’è¶…æ—¶ï¼Œè¶…æ—¶åè‡ªåŠ¨è§¦å‘ä¿åº•
            )

            if resp.status_code != 200:
                raise Exception(f"HTTP {resp.status_code}")

            soup = BeautifulSoup(resp.text, "html.parser")
            articles = soup.select("article.Box-row")

            if not articles:
                raise Exception("é¡µé¢ç»“æ„æ”¹å˜æˆ–ä¸ºç©º")

            repos = []
            for article in articles:
                try:
                    h2 = article.select_one("h2")
                    name = h2.select_one("a").get("href").strip("/")

                    p = article.select_one("p.col-9")
                    desc = p.text.strip() if p else "No description"

                    lang = article.select_one("[itemprop='programmingLanguage']")
                    language = lang.text.strip() if lang else "Other"

                    # æ€»æ˜Ÿæ•°
                    total_stars = 0
                    for a in article.select("a.Link--muted"):
                        if "stargazers" in a.get("href", ""):
                            total_stars = parse_stars_text(a.text)

                    # å‘¨æœŸå¢é‡ (Today/Weekly stars)
                    period_stars = 0
                    span = article.select_one("span.d-inline-block.float-sm-right")
                    if span:
                        period_stars = parse_stars_text(span.text.split(" stars")[0])

                    repos.append(
                        {
                            "name": name,
                            "language": language,
                            "stars_total": total_stars,
                            "stars_period": period_stars,
                            "url": f"https://github.com/{name}",
                            "desc": desc,
                            "source": "scrape",
                        }
                    )
                except:
                    continue

            if repos:
                print(f"  âœ… [Engine-A] {period} çˆ¬å–æˆåŠŸ ({len(repos)}æ¡)")
                return repos

        except Exception as e:
            print(f"  âš ï¸ [Engine-A] {period} å°è¯• {attempt} å¤±è´¥: {str(e)[:50]}")

    return None  # å¤±è´¥è¿”å› Noneï¼Œè§¦å‘ API ä¿åº•


# --- 4. æ ¸å¿ƒå¼•æ“ B: API ä¿åº• (é«˜å¯ç”¨) ---
def engine_api_fallback(period):
    """
    å½“çˆ¬è™«å¤±è´¥æ—¶ï¼Œä½¿ç”¨ API è·å–æ•°æ®
    ä¼˜åŠ¿ï¼šæå…¶ç¨³å®šï¼Œåªè¦ä»£ç†é€šå°±èƒ½è·å–
    ç­–ç•¥ï¼šæ··åˆæŸ¥è¯¢ 'æ–°åˆ›å»ºé¡¹ç›®' å’Œ 'æ´»è·ƒè€é¡¹ç›®'
    """
    print(f"[Engine-B] å¯åŠ¨ API ä¿åº•: {period}...")

    # æ„é€ æ™ºèƒ½æŸ¥è¯¢æ¡ä»¶
    now = datetime.datetime.now()
    queries = []

    if period == "daily":
        since = (now - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        # æŸ¥æ–°å»º + æŸ¥æ´»è·ƒ(è¿‘æœŸæœ‰æäº¤ä¸”æ˜Ÿæ•°é«˜)
        queries = [f"created:>{since}", f"pushed:>{since} stars:>1000"]
    elif period == "weekly":
        since = (now - datetime.timedelta(days=7)).strftime("%Y-%m-%d")
        queries = [f"created:>{since}", f"pushed:>{since} stars:>1000"]
    elif period == "monthly":
        since = (now - datetime.timedelta(days=30)).strftime("%Y-%m-%d")
        queries = [f"created:>{since}", f"pushed:>{since} stars:>2000"]
    elif period == "yearly":
        since = (now - datetime.timedelta(days=365)).strftime("%Y-%m-%d")
        queries = [f"created:>{since} stars:>500"]
    else:  # all time
        queries = ["stars:>2000"]

    url = "https://api.github.com/search/repositories"
    all_repos = {}  # ä½¿ç”¨å­—å…¸å»é‡

    for q in queries:
        try:
            params = {"q": q, "sort": "stars", "order": "desc", "per_page": 15}
            # API è¯·æ±‚å¢åŠ  30ç§’ è¶…æ—¶ï¼Œä¿è¯ç¨³å®šæ€§
            resp = requests.get(
                url,
                params=params,
                headers={"User-Agent": get_random_ua()},
                proxies=get_proxies(),
                timeout=30,
            )

            if resp.status_code == 200:
                items = resp.json().get("items", [])
                for item in items:
                    name = item.get("full_name")
                    if name and name not in all_repos:
                        # ä¼°ç®—å¢é‡é€»è¾‘ï¼š
                        # API æ— æ³•ç›´æ¥ç»™å¢é‡ã€‚å¦‚æœæ˜¯æ–°é¡¹ç›®ï¼Œå‡è®¾ å¢é‡=æ€»é‡ã€‚
                        # å¦‚æœæ˜¯è€é¡¹ç›®ï¼Œå¢é‡è®¾ä¸º 0 (å‰ç«¯ä¼šéšè— +0ï¼Œåªæ˜¾ç¤ºæ€»é‡ï¼Œäº¤äº’æ›´å‹å¥½)
                        is_new = False
                        if item.get("created_at"):
                            c_date = item["created_at"][:10]
                            # å¦‚æœåˆ›å»ºæ—¶é—´åœ¨æŸ¥è¯¢çª—å£å†…ï¼Œè§†ä¸ºæ–°é¡¹ç›®
                            if period == "daily" and c_date >= (
                                now - datetime.timedelta(days=1)
                            ).strftime("%Y-%m-%d"):
                                is_new = True
                            if period == "weekly" and c_date >= (
                                now - datetime.timedelta(days=7)
                            ).strftime("%Y-%m-%d"):
                                is_new = True

                        stars_total = item.get("stargazers_count", 0)

                        all_repos[name] = {
                            "name": name,
                            "language": item.get("language") or "Other",
                            "stars_total": stars_total,
                            "stars_period": stars_total if is_new else 0,
                            "url": item.get("html_url"),
                            "desc": item.get("description"),
                            "source": "api_fallback",
                        }
            else:
                print(f"  âš ï¸ [Engine-B] {period} API Error: {resp.status_code}")

        except Exception as e:
            print(f"  âš ï¸ [Engine-B] {period} æŸ¥è¯¢å¤±è´¥: {e}")

    # ç»“æœæ’åºï¼šä¼˜å…ˆå±•ç¤ºæœ‰å¢é‡çš„æ–°é¡¹ç›®ï¼Œå…¶æ¬¡æ˜¯é«˜æ˜Ÿè€é¡¹ç›®
    results = list(all_repos.values())
    results.sort(key=lambda x: (x["stars_period"], x["stars_total"]), reverse=True)

    if results:
        print(f"  âœ… [Engine-B] {period} ä¿åº•æˆåŠŸ ({len(results)}æ¡)")
    else:
        print(f"  âŒ [Engine-B] {period} è·å–å¤±è´¥ (ç½‘ç»œå…¨æ–­?)")

    return results


# --- 5. ä»»åŠ¡è°ƒåº¦ ---
def collect_task(period):
    data = []

    # ç­–ç•¥è·¯ç”±ï¼š
    # æ—¥/å‘¨/æœˆæ¦œ -> ä¼˜å…ˆçˆ¬è™« -> å¤±è´¥åˆ™ API
    if period in ["daily", "weekly", "monthly"]:
        data = engine_scrape(period)
        if not data:  # å¦‚æœçˆ¬è™«è¿”å› None
            data = engine_api_fallback(period)

    # å¹´/æ€»æ¦œ -> ç›´æ¥ API (GitHub å®˜ç½‘æ²¡æœ‰è¿™ä¸¤ä¸ªç»´åº¦çš„ Trending é¡µé¢)
    else:
        data = engine_api_fallback(period)

    return period, data or []


def save_json(data, period):
    if not data:
        return
    filename = f"github_{period}_{DATE_STR}.json"
    filepath = os.path.join(DATA_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"  ğŸ’¾ [Save] {period} ä¿å­˜å®Œæ¯•")


def main():
    print(f"========== ä»»åŠ¡å¼€å§‹ {DATE_STR} ==========")
    start_time = time.time()

    periods = ["daily", "weekly", "monthly", "yearly", "all"]

    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_p = {executor.submit(collect_task, p): p for p in periods}

        for future in as_completed(future_to_p):
            period, data = future.result()
            save_json(data, period)

    print(f"========== ä»»åŠ¡ç»“æŸ (è€—æ—¶ {time.time() - start_time:.2f}s) ==========")


if __name__ == "__main__":
    main()
